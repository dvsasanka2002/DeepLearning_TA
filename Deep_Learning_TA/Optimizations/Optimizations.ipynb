{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQa4uX8tunP4","executionInfo":{"status":"ok","timestamp":1694416623850,"user_tz":-330,"elapsed":25582,"user":{"displayName":"DV sasanka","userId":"17421081062846106801"}},"outputId":"bd752234-43e5-4f36-9b89-b0316d85fd0d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io\n","import math\n","import sklearn\n","import sklearn.datasets\n","import sys\n","sys.path.append('/content/drive/MyDrive/Deep_Learning_TA/Optimizations/')\n","from utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n","from utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset"],"metadata":{"id":"XPAgmrxEuhed","colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"status":"error","timestamp":1694416004874,"user_tz":-330,"elapsed":5,"user":{"displayName":"paresh saxena","userId":"02313003228442574387"}},"outputId":"0bde4324-54f3-490b-e29a-b696cbd2f2b2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-eba1585926d4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Deep_Learning_TA/Optimizations/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_params_and_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"metadata":{"id":"e1oDQV2oyx24"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMrC9JiihVZG"},"outputs":[],"source":["train_X, train_Y = load_dataset()\n","print(train_X.shape)"]},{"cell_type":"code","source":["def update_parameters_with_gd(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using one step of gradient descent\n","\n","    Arguments:\n","    parameters -- python dictionary containing your parameters to be updated:\n","                    parameters['W' + str(l)] = Wl\n","                    parameters['b' + str(l)] = bl\n","    grads -- python dictionary containing your gradients to update each parameters:\n","                    grads['dW' + str(l)] = dWl\n","                    grads['db' + str(l)] = dbl\n","    learning_rate -- the learning rate, scalar.\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters\n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers in the neural networks\n","\n","    # Update rule for each parameter\n","    for l in range(L):\n","\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads['dW' + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads['db' + str(l+1)]\n","\n","    return parameters"],"metadata":{"id":"Bw9ApgJ_0wHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_velocity(parameters):\n","    \"\"\"\n","    Initializes the velocity as a python dictionary with:\n","                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n","                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n","    Arguments:\n","    parameters -- python dictionary containing your parameters.\n","                    parameters['W' + str(l)] = Wl\n","                    parameters['b' + str(l)] = bl\n","\n","    Returns:\n","    v -- python dictionary containing the current velocity.\n","                    v['dW' + str(l)] = velocity of dWl\n","                    v['db' + str(l)] = velocity of dbl\n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers in the neural networks\n","    v = {}\n","\n","    # Initialize velocity\n","    for l in range(L):\n","\n","        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n","        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n","\n","    return v"],"metadata":{"id":"X5k5oM-x03xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_adam(parameters) :\n","    \"\"\"\n","    Initializes v and s as two python dictionaries with:\n","                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n","                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n","\n","    Arguments:\n","    parameters -- python dictionary containing your parameters.\n","                    parameters[\"W\" + str(l)] = Wl\n","                    parameters[\"b\" + str(l)] = bl\n","\n","    Returns:\n","    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n","                    v[\"dW\" + str(l)] = ...\n","                    v[\"db\" + str(l)] = ...\n","    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n","                    s[\"dW\" + str(l)] = ...\n","                    s[\"db\" + str(l)] = ...\n","\n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers in the neural networks\n","    v = {}\n","    s = {}\n","\n","    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n","    for l in range(L):\n","\n","        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n","        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n","        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n","        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n","\n","    return v, s"],"metadata":{"id":"kQ9X79vS095j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n","    \"\"\"\n","    Update parameters using Momentum\n","\n","    Arguments:\n","    parameters -- python dictionary containing your parameters:\n","                    parameters['W' + str(l)] = Wl\n","                    parameters['b' + str(l)] = bl\n","    grads -- python dictionary containing your gradients for each parameters:\n","                    grads['dW' + str(l)] = dWl\n","                    grads['db' + str(l)] = dbl\n","    v -- python dictionary containing the current velocity:\n","                    v['dW' + str(l)] = ...\n","                    v['db' + str(l)] = ...\n","    beta -- the momentum hyperparameter, scalar\n","    learning_rate -- the learning rate, scalar\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters\n","    v -- python dictionary containing your updated velocities\n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers in the neural networks\n","\n","    # Momentum update for each parameter\n","    for l in range(L):\n","\n","        # compute velocities\n","        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)] + (1 - beta)*grads['dW' + str(l+1)]\n","        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)] + (1 - beta)*grads['db' + str(l+1)]\n","        # update parameters\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v[\"db\" + str(l+1)]\n","\n","    return parameters, v"],"metadata":{"id":"GKd07uoe1HzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n","                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n","    \"\"\"\n","    Update parameters using Adam\n","\n","    Arguments:\n","    parameters -- python dictionary containing your parameters:\n","                    parameters['W' + str(l)] = Wl\n","                    parameters['b' + str(l)] = bl\n","    grads -- python dictionary containing your gradients for each parameters:\n","                    grads['dW' + str(l)] = dWl\n","                    grads['db' + str(l)] = dbl\n","    v -- Adam variable, moving average of the first gradient, python dictionary\n","    s -- Adam variable, moving average of the squared gradient, python dictionary\n","    learning_rate -- the learning rate, scalar.\n","    beta1 -- Exponential decay hyperparameter for the first moment estimates\n","    beta2 -- Exponential decay hyperparameter for the second moment estimates\n","    epsilon -- hyperparameter preventing division by zero in Adam updates\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters\n","    v -- Adam variable, moving average of the first gradient, python dictionary\n","    s -- Adam variable, moving average of the squared gradient, python dictionary\n","    \"\"\"\n","\n","    L = len(parameters) // 2                 # number of layers in the neural networks\n","    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n","    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n","\n","    # Perform Adam update on all parameters\n","    for l in range(L):\n","        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n","\n","        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)] + (1 - beta1)*grads['dW' + str(l+1)]\n","        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)] + (1 - beta1)*grads['db' + str(l+1)]\n","\n","        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n","\n","        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1 - beta1**t)\n","        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1 - beta1**t)\n","\n","\n","        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n","\n","        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)] + (1 - beta2)*np.square(grads['dW' + str(l+1)])\n","        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)] + (1 - beta2)*np.square(grads['db' + str(l+1)])\n","\n","\n","        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n","\n","        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1 - beta2**t)\n","        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1 - beta2**t)\n","\n","        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n","\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n","\n","\n","    return parameters, v, s"],"metadata":{"id":"fRk7Pmxz1Psq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_adagrad(parameters):\n","    \"\"\"\n","    Initializes the variables for AdaGrad optimizer.\n","\n","    Arguments:\n","    parameters -- python dictionary containing the parameters to be optimized.\n","\n","    Returns:\n","    v -- python dictionary containing the accumulated squared gradient for each parameter.\n","    \"\"\"\n","    L = len(parameters) // 2\n","    v = {}\n","\n","    for l in range(L):\n","        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n","        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n","\n","    return v\n"],"metadata":{"id":"2Sd8-k57CbdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def update_parameters_with_adagrad(parameters, grads, v, learning_rate, epsilon=1e-8):\n","    \"\"\"\n","    Update parameters using AdaGrad optimizer.\n","\n","    Arguments:\n","    parameters -- python dictionary containing the parameters to be optimized.\n","    grads -- python dictionary containing the gradients of the cost function.\n","    v -- python dictionary containing the accumulated squared gradient for each parameter.\n","    learning_rate -- the learning rate, scalar.\n","    epsilon -- hyperparameter preventing division by zero.\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters.\n","    \"\"\"\n","    L = len(parameters) // 2\n","\n","    for l in range(L):\n","        v[\"dW\" + str(l + 1)] += grads[\"dW\" + str(l + 1)] ** 2\n","        v[\"db\" + str(l + 1)] += grads[\"db\" + str(l + 1)] ** 2\n","\n","        parameters[\"W\" + str(l + 1)] -= (learning_rate / (np.sqrt(v[\"dW\" + str(l + 1)]) + epsilon)) * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] -= (learning_rate / (np.sqrt(v[\"db\" + str(l + 1)]) + epsilon)) * grads[\"db\" + str(l + 1)]\n","\n","    return parameters,v"],"metadata":{"id":"aVvL7RtPCc1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_rmsprop(parameters):\n","    \"\"\"\n","    Initializes the variables for RMSprop optimizer.\n","\n","    Arguments:\n","    parameters -- python dictionary containing the parameters to be optimized.\n","\n","    Returns:\n","    s -- python dictionary containing the moving average of squared gradient for each parameter.\n","    \"\"\"\n","    L = len(parameters) // 2\n","    s = {}\n","\n","    for l in range(L):\n","        s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n","        s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n","\n","    return s"],"metadata":{"id":"NhCvDldNChzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def update_parameters_with_rmsprop(parameters, grads, s, learning_rate, beta=0.9, epsilon=1e-8):\n","    \"\"\"\n","    Update parameters using RMSprop optimizer.\n","\n","    Arguments:\n","    parameters -- python dictionary containing the parameters to be optimized.\n","    grads -- python dictionary containing the gradients of the cost function.\n","    s -- python dictionary containing the moving average of squared gradient for each parameter.\n","    learning_rate -- the learning rate, scalar.\n","    beta -- the decay rate for the moving average (usually 0.9).\n","    epsilon -- hyperparameter preventing division by zero.\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters.\n","    \"\"\"\n","    L = len(parameters) // 2\n","\n","    for l in range(L):\n","        s[\"dW\" + str(l + 1)] = beta * s[\"dW\" + str(l + 1)] + (1 - beta) * (grads[\"dW\" + str(l + 1)] ** 2)\n","        s[\"db\" + str(l + 1)] = beta * s[\"db\" + str(l + 1)] + (1 - beta) * (grads[\"db\" + str(l + 1)] ** 2)\n","\n","        parameters[\"W\" + str(l + 1)] -= (learning_rate / (np.sqrt(s[\"dW\" + str(l + 1)]) + epsilon)) * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] -= (learning_rate / (np.sqrt(s[\"db\" + str(l + 1)]) + epsilon)) * grads[\"db\" + str(l + 1)]\n","\n","    return parameters,s"],"metadata":{"id":"FBmUinlNClMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, beta=0.9,\n","          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n","    \"\"\"\n","    3-layer neural network model trained using full batch gradient descent.\n","\n","    Arguments:\n","    X -- input data, of shape (n_features, number of examples)\n","    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n","    layers_dims -- python list, containing the size of each layer\n","    learning_rate -- the learning rate, scalar.\n","    beta -- Momentum hyperparameter\n","    beta1 -- Exponential decay hyperparameter for the past gradients estimates\n","    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates\n","    epsilon -- hyperparameter preventing division by zero in Adam updates\n","    num_epochs -- number of epochs\n","    print_cost -- True to print the cost every 1000 epochs\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters\n","    \"\"\"\n","\n","    L = len(layers_dims)  # number of layers in the neural networks\n","    costs = []  # to keep track of the cost\n","    t = 0  # initializing the counter required for Adam update\n","    m = X.shape[1]  # number of training examples\n","\n","    # Initialize parameters\n","    parameters = initialize_parameters(layers_dims)\n","\n","    # Initialize the optimizer\n","    if optimizer == \"gd\":\n","        pass  # no initialization required for gradient descent\n","    elif optimizer == \"momentum\":\n","        v = initialize_velocity(parameters)\n","    elif optimizer == \"adam\":\n","        v, s = initialize_adam(parameters)\n","    elif optimizer == \"rmsprop\":\n","      s = initialize_rmsprop(parameters)\n","    elif optimizer == \"adagrad\":\n","      v = initialize_adagrad(parameters)\n","\n","\n","    # Optimization loop\n","    for i in range(num_epochs):\n","\n","        # Forward propagation\n","        a3, caches = forward_propagation(X, parameters)\n","\n","        # Compute cost\n","        cost = compute_cost(a3, Y)\n","\n","        # Backward propagation\n","        grads = backward_propagation(X, Y, caches)\n","\n","        # Update parameters\n","        if optimizer == \"gd\":\n","            parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n","        elif optimizer == \"momentum\":\n","            parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n","        elif optimizer == \"adam\":\n","            t = t + 1  # Adam counter\n","            parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n","                                                           t, learning_rate, beta1, beta2, epsilon)\n","        elif optimizer == \"rmsprop\":\n","            parameters,s = update_parameters_with_rmsprop(parameters, grads, s, learning_rate, beta=0.9, epsilon=1e-8)\n","        elif optimizer == \"adagrad\":\n","            parameters,v = update_parameters_with_adagrad(parameters, grads, v, learning_rate, epsilon=1e-8)\n","\n","\n","        # Print the cost every 1000 epochs\n","        if print_cost and i % 1000 == 0:\n","            print(\"Cost after epoch %i: %f\" % (i, cost))\n","        if print_cost and i % 100 == 0:\n","            costs.append(cost)\n","\n","    # Plot the cost\n","    plt.plot(costs)\n","    plt.ylabel('Cost')\n","    plt.xlabel('Epochs (per 1000)')\n","    plt.title(\"Learning rate = \" + str(learning_rate))\n","    plt.show()\n","\n","    return parameters\n"],"metadata":{"id":"A5QjUhLVhyFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers_dims = [train_X.shape[0], 10, 4, 1]\n","parameters = model(train_X, train_Y, layers_dims,\"gd\",0.005)\n","\n","# Predict\n","predictions = predict(train_X, train_Y, parameters)\n","\n","# Plot decision boundary\n","plt.title(\"Model with Gradient Descent optimization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5,2.5])\n","axes.set_ylim([-1,1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"metadata":{"id":"CaPkxdKRy7x2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers_dims = [train_X.shape[0], 10, 4, 1]\n","parameters = model(train_X, train_Y, layers_dims,\"momentum\",0.005,0.99)\n","\n","# Predict\n","predictions = predict(train_X, train_Y, parameters)\n","\n","# Plot decision boundary\n","plt.title(\"Model with Momentum optimization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5,2.5])\n","axes.set_ylim([-1,1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"metadata":{"id":"czJF4Fm5zsAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train 3-layer model\n","layers_dims = [train_X.shape[0], 10, 4, 1]\n","parameters = model(train_X, train_Y, layers_dims,\"adam\",0.005)\n","\n","# Predict\n","predictions = predict(train_X, train_Y, parameters)\n","\n","# Plot decision boundary\n","plt.title(\"Model with Adam optimization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5,2.5])\n","axes.set_ylim([-1,1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"metadata":{"id":"VI3jk5XA0osz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train 3-layer model\n","layers_dims = [train_X.shape[0], 10, 4, 1]\n","parameters = model(train_X, train_Y, layers_dims,\"adagrad\",0.005)\n","\n","# Predict\n","predictions = predict(train_X, train_Y, parameters)\n","\n","# Plot decision boundary\n","plt.title(\"Model with Adagrad optimization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5,2.5])\n","axes.set_ylim([-1,1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"metadata":{"id":"HfgOVu301foQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train 3-layer model\n","layers_dims = [train_X.shape[0], 10, 4, 1]\n","parameters = model(train_X, train_Y, layers_dims,\"rmsprop\",0.005)\n","\n","# Predict\n","predictions = predict(train_X, train_Y, parameters)\n","\n","# Plot decision boundary\n","plt.title(\"Model with RMSProp optimization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5,2.5])\n","axes.set_ylim([-1,1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"metadata":{"id":"kR4LQRNIERSw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_parameters_with_batch_norm(layers_dims):\n","    \"\"\"\n","    Initializes parameters for a neural network with batch normalization.\n","\n","    Arguments:\n","    layers_dims -- python list, containing the size of each layer.\n","\n","    Returns:\n","    parameters -- python dictionary containing the initialized parameters.\n","    \"\"\"\n","    L = len(layers_dims) - 1\n","    parameters = {}\n","    for l in range(1, L + 1):\n","        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n","        parameters['gamma' + str(l)] = np.ones((layers_dims[l], 1))\n","        parameters['beta' + str(l)] = np.zeros((layers_dims[l], 1))\n","    return parameters\n","\n","\n","\n","def compute_cost_with_batch_norm(AL, Y):\n","    \"\"\"\n","    Computes the cost for a neural network with batch normalization.\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples).\n","    Y -- true \"label\" vector, shape (1, number of examples).\n","\n","    Returns:\n","    cost -- cross-entropy cost.\n","    \"\"\"\n","    m = Y.shape[1]\n","    cost = (-1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n","    return cost\n"],"metadata":{"id":"kviocBwmEvcw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def forward_propagation_with_batch_norm(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation with batch normalization.\n","\n","    Arguments:\n","    X -- input data, of shape (n_features, number of examples).\n","    parameters -- python dictionary containing the parameters.\n","\n","    Returns:\n","    A -- output of the last layer.\n","    caches -- list of caches containing all the intermediate values.\n","    \"\"\"\n","    caches = []\n","    A = X\n","    L = len(parameters) // 4\n","\n","    for l in range(1, L):\n","        Z, Z_centered, gamma, beta, mu, sigma_sq = batch_norm_forward(parameters['W' + str(l)], parameters['b' + str(l)],\n","                                                                     parameters['gamma' + str(l)], parameters['beta' + str(l)], A)\n","        A = relu(Z)\n","        cache = (Z, Z_centered, gamma, beta, mu, sigma_sq, A)\n","        caches.append(cache)\n","\n","    Z, Z_centered, gamma, beta, mu, sigma_sq = batch_norm_forward(parameters['W' + str(L)], parameters['b' + str(L)],\n","                                                                 parameters['gamma' + str(L)], parameters['beta' + str(L)], A)\n","    AL = sigmoid(Z)\n","    cache = (Z, Z_centered, gamma, beta, mu, sigma_sq, AL)\n","    caches.append(cache)\n","\n","    return AL, caches"],"metadata":{"id":"u3MVKAbfGUte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def backward_propagation_with_batch_norm(AL, Y, caches):\n","    \"\"\"\n","    Implements the backward propagation with batch normalization.\n","\n","    Arguments:\n","    AL -- output of the last layer.\n","    Y -- true \"label\" vector.\n","    caches -- list of caches containing intermediate values.\n","\n","    Returns:\n","    grads -- dictionary containing gradients with respect to different parameters.\n","    \"\"\"\n","    grads = {}\n","    L = len(caches)\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape)\n","\n","    # Backward propagation for the last layer\n","    dZL = AL - Y\n","    grads[\"dW\" + str(L)], grads[\"db\" + str(L)], grads[\"dgamma\" + str(L)], grads[\"dbeta\" + str(L)] = \\\n","        batch_norm_backward(dZL, caches[L - 1])\n","    grads[\"dA\" + str(L - 1)] = np.dot(caches[L - 1][0], grads[\"dZ\" + str(L)])\n","\n","    # Backward propagation for hidden layers\n","    for l in reversed(range(1, L)):\n","        dA_prev_temp = np.dot(grads[\"dZ\" + str(l + 1)], caches[l][0].T)\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        dZ_temp, grads[\"dW\" + str(l)], grads[\"db\" + str(l)], grads[\"dgamma\" + str(l)], grads[\"dbeta\" + str(l)] = \\\n","            batch_norm_backward(grads[\"dA\" + str(l)], caches[l - 1])\n","        grads[\"dA\" + str(l - 1)] = np.dot(caches[l - 1][0], dZ_temp)\n","\n","    return grads"],"metadata":{"id":"poDc8SoNGVOb"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}